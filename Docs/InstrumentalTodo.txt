- Get a simple resizable panel working.
- Create a keyvalue/pair binding list for mapping schema controls to runtime controls.
	- Upon loading a schema (editor or runtime), populate the runtime control list with proper data from the schema.
	
- Fillet panel struggles with no radius segments, it's a triangle index bug of some kind.
- Depth, and extrude handles. Figure out how to change mode of fillet panel to allow extrude
- use RED chan as a color palette inspiration
- Color picker for panel background and outlines
- outline and extrude fails if you don't also do back face.
- Add ability to move the work table height.

- Square panel can use leap outline graphic for outlines
	(there doesn't seem to be a non graphic renderer equivalent though - make one of our own?)
- Figure out how to support multiple colliders on a placeholder object. Currently they cause the space switching to break.
- Ignore collisions between work table and panel to prevent bouncing and popping

Design for color dropper and all other tools:
- InventoryMarkerClass
	- Lets you store controls as well as tools.
- Goes in wrist-mounted anchor slots
- handheld tools can have a menu pulled out of them keiichi slider style
	- In fact, let's use that for the tools menu too.
	- Oh maybe the tools menu can be a rotating thing - like a store jewelry display that spins in place with multiple faces. This would let us stick it to the control palette
	
- Study '3d text' from 98 to early 00's to see how they kept things readable even with extrusions. Might not be too helpful though because all examples from then were fixed-perspective, whereas VR is continuously variable perspective. (One advancement we've made recently in this area is the skate-frame extrude. Extrude upwards, inset the border faces w/o profile change, then extrude those inset faces outwards). It looks like the most important thing for readability is the strength of the silhouette - this gets muddied if the extruded section seamlessly blends into the face section. We can still have depth information here though - make the extruded section a different 'outline' color but it can have a depth-based gradient to it.
	
Pull-out heirarchy:
- Main
	-Panel (This gets stretched)
	-Slider
		-Ball Object
	-Name (show when hovering or looking)
	
	Slider will need some kind of return to center, as well as 'lock to edges' behavior. Perhaps we can do this by directly setting the slider values?
	
- graphic group index and connected renderer appear to be getting corrupted in HandleRuntimeAddRemove() ?
	- Looks like HandleRuntimeAddRemove is getting run twice?
	- (because detaching and attaching operations get bucketed, it looks like detach is getting called after attach. This might be a bug worthy of reporting. A workaround is to distribute these operations across frames)
	
- Graphic group switching runs into issues if you go quickly from palette to panel, works if you slowly go through each zone

- placement controls need to stick to UI panels (Either go kinematic or do force trickery) after they've been placed
	- make it so that you can throw controls onto UI panels. This is dumb but it should be supported


https://github.com/sigtrapgames/SmartData use this for letting users do data connections?
In the intervening years since starting this project, I've learned that React has a nice pseudo-immediate mode system that people really like. Look into providing that here.

Min and max dimensions:
max: x: 0.9, y: 0.6
min: x: 0.09, y: 0.09

Floating object values:
Mass: 1
drag: 8
Angular Drag: 16

-----------------------------------
Button edgeloop:
corner segment is cornerVertCount

0-cornerVertCount: upper left
cornerVertCount-widthCount: upper
widthCount-widthCount+cornerVertCount: upperRight

ok looks like I made a silly mistake - the face of the button is at 0 depth, and the back of the button gets pushed into the negative range. It might not be _that_ silly, there's possible logic to it, such as being able to check if the finger is touching by looking to see if the value is negative, but I think I want to fix that now.

New notes: It looks like I had not finished getting the button model rendering properly, and the legacy buttons were thrown together quickly to solve this.

-----------------------------------
Graphics stuff:
I need to figure out the best way to handle vertex colors and coloration. I want to be able to quickly change colors of certain elements every frame (such as button faces), but doing that per-vertex might be a bad idea. I was thinking we could possibly put the shading value into the vertex colors, and then use a palette system for coloring things dynamically like the hover and touch.

It does look like I can use a similar palette system to the one I'm familiar with. There doesn't seem to be anything in the schema right now for color definitions, so I've got a blank slate to play with there. Ugh the theming is a lot to think about and I kinda don't wanna lol I need to start moving faster too. Hmm. It does look like on the fillet panel at least, we generate the vcolors on generate mesh, and don't modify them during setvertices

Let's provide base colors in the vertex colors. We can have shader common values for:
- Touch Start
- Touch complete
- Hover (far - interaction point is outside of the bounds but nearby)
- Hover (near - interaction point is inside of the bounds)
- Press
- Grasp

Global shader values for:
- interaction positions (left/right)

Material property block data for:
- Touch Amount
- IsPressing
- IsGrasping

- Also the ability to do cubemaps for shiny stuff

-----------------------------------
Button interaction:
I've got a bit of a problem. I've got the old fingerbutton class, and that's all well and good and all. But do I really want to have 'button' and 'fingerbutton' as separate classes? It kinda makes sense - we can instantiate and config FingerButton at runtime if we're in runtime mode, and leave it out if we're in design mode. Would still be nice to have the collider, but meh duplicating the collider generation isn't that big of a deal. I guess I didn't really think through having design time and runtime live along side eachother very much during the intial phase, figured I'd just yolo it. And fingerbutton? What if we've got other button pressing activation interactions? What about foot pressing? Eye pressing (weird but not impossible, esp with multimodal. What about distance pressing?) Not a pressing matter for right now (lololol)

If we did do the generic thing, we could split it up like this:
store a list of button pressor positions
These can be vector3s. xy for 2d position, and z for normalized depth
We can have standard behavior for fully 3d pressors - fingers, feet, whatever.
For pseudo 3d pressors like gaze dots, we can handle the xy like normal, but map the depth amount to a separate value, like a pinch amount for a remote pinch raycast, 

Looking at how UIControl is currently setup. I wonder if we can set up proper handling for the difference between design time behavior, palette behavior, and runtime behavior. It's already an abstract class, but I wonder if we can have abstract handlers or something? idk the hacky way right now seems to be to have separate components for each mode. It looks like all UIControls look for a panel in their parent transform heirarchy. Notable in that we don't have any existing examples of this except the Panel object in the editing studio - the one that is the target of edit mode. Looks like Panel is always set to isDesignerMode, which is definitely a limitation. I had clearly originally made it so that only this editor could author panels and controls. Blegh, this had only been set up for the old way.

I wonder if I can make it so that schema are scriptable objects. Would be nice to have json serialization/deserialization as well. Scriptable objects would mean we can make individual elements without needing the whole editor suite working (it will be awhile before that thing works properly again tbh desu). Oh, the root uischema data is already a scriptable object. I should honestly just convert the rest of them and call it a day - there seem to be tools for handling serialization/deserialzation as json too. I've got the old project as reference for anything that gets blown away. Nice, I think this will work!

Interesting that uischema would have the ability to hold controls with no panel. Makes sense I guess? If we wanted to have free-floating controls. Makes me wonder if that was the intention or if things were just not thought through very well. I can see it being useful though, so I might keep it.

I'm looking at UICommonElements, which is where we'd normally store all of our prefabs for spawning. There does seem to be one problem though: unity serialization doesn't like dictionaries. We can probably just turn this into an array and call it a day nbd.

Hmm. Need to think about the collider situation. For perfectly round buttons it's fine to have a capsule collider as the trigger region, but as soon as the button can go wide, no such collider is acceptable. I suppose it'd be possible to have 3 colliders, 2 capsules and a connecting box but that feels weird - we'd have to write pretty hacky 'in region' code. I suppose we could do a convex mesh collider but eh, performance and runtime updating. Don't like it. We'd also be giving up our ability to start the hover on trigger enter. Guess I'll just do box colliders for both.

Looking at the buttonmodel class, we'll need to supply accessors so that the owning class (button) can send it proper updates based off of schema data. Oh hmm it looks like methods are used instead of accessor fields. Not a huge difference tbh. Interesting though that we provide const float constraints. One thing that does kind of suck though is that in the editor for design at least, it's a good idea to be able to have something where we can set sliders and have them automatically update the visual representation. Maybe this means writing a custom editor for the schema itself, although our lack of inheritance means we can't exactly expose all the right options. We'll also need to start looking into instancing here.

I just realized that with the right editors I can make dealing with controls and schema easier. Have a reference to the schema asset (will want to make button schema and other sub-classes into scriptable objects too), then have a serializedobject inspector that can edit them in-situ, so we don't have to click out to the asset to modify. Changes can be detected and propagated to the control, even at edit time. Also, thinking about changing how the generation is done a bit. One thing I'm a bit worried about is the inability to test things discretely - for now I can test a buttonmodel script by itself, but what if I change the generation? I'm also a bit worried about having the mesh re-generation. Hmm, actually idk what we even do wit hthe properties changed method, but it would probably be a good idea to make a batch property changing method so we can avoid re-generating everything. maybe change OnValidate's behavior if in play mode?

OK, starting to think about the best way forward here - I think what I can do is I can have PropertiesChanged offer up both a new and old buttonSchema. We can compare the two each time there's an update so we know exactly what changed.

I should really improve these sfx sometime. I've deleted the ones that really shouldn't be in there and added some replacements.

I think this version of TweenMaterialColor is old, so I might need to update it a bit to support material property blocks. Will get to that at some point though. I'm tempted to break out the visual parts a bit more, idk. How does 'buttonruntimevisual' sound? Or maybe ButtonUnityGraphic can have Runtime checking in its Update() method?

------------------------------------
Solving the 'tightly packed gizmos' issue:
perhaps we can have a single ball-shaped gizmo that expands when hovered near, into a radial segment of other balls. The other balls can show axial constraints and names as you get closer to them. (Maybe we can do this craigslist-maps style, where the number of gizmos underneath can be a number inside of a ball)